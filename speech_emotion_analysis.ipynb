{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182c77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from audio\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, res_type='polyphase', duration=2.5, sr=22050*2, offset=0.5)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "    return np.mean(mfccs.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1f5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load audio data and labels\n",
    "def load_data(data_folder='Dataset'):\n",
    "    audio_data = []\n",
    "    labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(data_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Extract emotion label from the filename\n",
    "                emotion = file.split(\"-\")[2]\n",
    "                labels.append(emotion)\n",
    "                audio_data.append(file_path)\n",
    "\n",
    "    return audio_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af363fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio data and labels\n",
    "audio_data, labels = load_data(data_folder='Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08d6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(audio_data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226a67fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resampy'\n\nThis error is lazily reported, having originally occured in\n  File b:\\Git hub\\Speech-Emotion-Analysis\\.env\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n\n----> resampy = lazy.load(\"resampy\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Feature extraction\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train = np.array([\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train])\n\u001b[32m      3\u001b[39m X_test = np.array([extract_features(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_test])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_features\u001b[39m(file_path):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     audio, sample_rate = \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkaiser_fast\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m22050\u001b[39;49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=\u001b[32m13\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(mfccs.T, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mb:\\Git hub\\Speech-Emotion-Analysis\\.env\\Lib\\site-packages\\librosa\\core\\audio.py:193\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    190\u001b[39m     y = to_mono(y)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     y = \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    196\u001b[39m     sr = sr_native\n",
      "\u001b[36mFile \u001b[39m\u001b[32mb:\\Git hub\\Speech-Emotion-Analysis\\.env\\Lib\\site-packages\\librosa\\core\\audio.py:678\u001b[39m, in \u001b[36mresample\u001b[39m\u001b[34m(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)\u001b[39m\n\u001b[32m    669\u001b[39m     y_hat = np.apply_along_axis(\n\u001b[32m    670\u001b[39m         soxr.resample,\n\u001b[32m    671\u001b[39m         axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    675\u001b[39m         quality=res_type,\n\u001b[32m    676\u001b[39m     )\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m     y_hat = \u001b[43mresampy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresample\u001b[49m(y, orig_sr, target_sr, \u001b[38;5;28mfilter\u001b[39m=res_type, axis=axis)\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[32m    681\u001b[39m     y_hat = util.fix_length(y_hat, size=n_samples, axis=axis, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mb:\\Git hub\\Speech-Emotion-Analysis\\.env\\Lib\\site-packages\\lazy_loader\\__init__.py:117\u001b[39m, in \u001b[36mDelayedImportErrorModule.__getattr__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m     fd = \u001b[38;5;28mself\u001b[39m.__frame_data\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m    118\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.__message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis error is lazily reported, having originally occured in\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  File \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[33m\"\u001b[39m\u001b[33mlineno\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    121\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m----> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(fd[\u001b[33m\"\u001b[39m\u001b[33mcode_context\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    122\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'resampy'\n\nThis error is lazily reported, having originally occured in\n  File b:\\Git hub\\Speech-Emotion-Analysis\\.env\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n\n----> resampy = lazy.load(\"resampy\")"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "X_train = np.array([extract_features(x) for x in X_train])\n",
    "X_test = np.array([extract_features(x) for x in X_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
